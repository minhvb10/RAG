# RAG
This is a basic question answering system using RAG. The input will be a PDF file and the output will be an answer that generated by the LLM after retrieved from the PDF file based on my question. In this project I used google/embeddinggemma-300m for embedding each chunk after spliting the pdf file into chunks, BAAI/bge-reranker-v2-m3 for reranking top 10 chunks after retrieved from the faiss file and gemini-2.5-pro for the generation. 
# Requirement
To use google/embeddinggemma-300m you must create a hugging face token and grant access to the "Google's Gemma models family" group. After that, you must create an API key from google ai studio for using the gemini-2.5-pro model. 
# Demo
